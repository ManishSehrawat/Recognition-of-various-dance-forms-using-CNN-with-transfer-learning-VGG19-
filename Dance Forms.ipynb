{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96.jpg</td>\n",
       "      <td>manipuri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163.jpg</td>\n",
       "      <td>bharatanatyam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>450.jpg</td>\n",
       "      <td>odissi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>219.jpg</td>\n",
       "      <td>kathakali</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>455.jpg</td>\n",
       "      <td>odissi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Image         target\n",
       "0   96.jpg       manipuri\n",
       "1  163.jpg  bharatanatyam\n",
       "2  450.jpg         odissi\n",
       "3  219.jpg      kathakali\n",
       "4  455.jpg         odissi"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>508.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>246.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>473.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>485.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Image\n",
       "0  508.jpg\n",
       "1  246.jpg\n",
       "2  473.jpg\n",
       "3  485.jpg\n",
       "4  128.jpg"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manipuri' 'bharatanatyam' 'odissi' 'kathakali' 'kathak' 'sattriya'\n",
      " 'kuchipudi' 'mohiniyattam']\n"
     ]
    }
   ],
   "source": [
    "print(train['target'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class_map={'manipuri':0, 'bharatanatyam':1, 'odissi':2 ,'kathakali':3, 'kathak':4, 'sattriya':5,\n",
    " 'kuchipudi':6, 'mohiniyattam':7}\n",
    "inverse_map={0:'manipuri', 1:'bharatanatyam', 2:'odissi' ,3:'kathakali',4: 'kathak', 5:'sattriya',\n",
    " 6:'kuchipudi', 7:'mohiniyattam'}\n",
    "train['target']=train['target'].map(Class_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>450.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>219.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>455.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Image  target\n",
       "0   96.jpg       0\n",
       "1  163.jpg       1\n",
       "2  450.jpg       2\n",
       "3  219.jpg       3\n",
       "4  455.jpg       2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_h,img_w= (224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 364/364 [00:03<00:00, 93.75it/s] \n"
     ]
    }
   ],
   "source": [
    "train_img=[]\n",
    "train_label=[]\n",
    "j=0\n",
    "path='dataset/train'\n",
    "for i in tqdm(train['Image']):\n",
    "    final_path=os.path.join(path,i)\n",
    "    img=cv2.imread(final_path)\n",
    "    img=cv2.resize(img,(img_h,img_w))\n",
    "    img=img.astype('float32')\n",
    "    train_img.append(img)\n",
    "    train_label.append(train['target'][j])\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_img, train_label, test_size=0.30, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:02<00:00, 77.55it/s]\n"
     ]
    }
   ],
   "source": [
    "test_img=[]\n",
    "path='dataset/test'\n",
    "for i in tqdm(test['Image']):\n",
    "    final_path=os.path.join(path,i)\n",
    "    img=cv2.imread(final_path)\n",
    "    img=cv2.resize(img,(img_h,img_w))\n",
    "    img=img.astype('float32')\n",
    "    test_img.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manish Sehrawat\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,# divide each input by its std\n",
    "        rescale=1./255,\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.3, # Randomly zoom image \n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "test_datagen= ImageDataGenerator(rescale=1./255)\n",
    "valid_datagen= ImageDataGenerator(rescale=1./255)\n",
    "train_datagen.fit(x_train)\n",
    "test_datagen.fit(test_img)\n",
    "valid_datagen.fit(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data= (254, 224, 224, 3)  and shape of labels of training data=  (254,)\n",
      "Shape of validation data= (110, 224, 224, 3)  and shape of labels of validation data=  (110,)\n",
      "Shape of test data= (156, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "train_img=np.array(train_img)\n",
    "x_train= np.array(x_train)\n",
    "x_valid= np.array(x_valid)\n",
    "y_train= np.array(y_train)\n",
    "y_valid= np.array(y_valid)\n",
    "test_img=np.array(test_img)\n",
    "train_label=np.array(train_label)\n",
    "print(\"Shape of training data=\",x_train.shape,\" and shape of labels of training data= \",y_train.shape)\n",
    "print(\"Shape of validation data=\",x_valid.shape,\" and shape of labels of validation data= \",y_valid.shape)\n",
    "print(\"Shape of test data=\",test_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16 \n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Lambda,Conv2D,MaxPooling2D,Flatten,Dense,Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.vgg19 import VGG19 \n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.optimizers import RMSprop,Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing import image\n",
    "vggmodel=VGG16(weights='imagenet',include_top=False,input_shape=(224,224,3),pooling='max')\n",
    "vggmodel.trainable=False\n",
    "model=Sequential([\n",
    "    vggmodel,\n",
    "    Dense(units=1024,activation='relu',kernel_initializer='uniform'),\n",
    "    Dropout(0.25),\n",
    "    Dense(units=512,activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    Dense(units=8,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 512)               20024384  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 20,176,008\n",
      "Trainable params: 4,869,576\n",
      "Non-trainable params: 15,306,432\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19,preprocess_input\n",
    "base_model_3=VGG19(include_top=False, weights='imagenet',input_shape=(img_h,img_w,3), pooling='max')\n",
    "\n",
    "for layer in base_model_3.layers[:-4]:\n",
    "    layer.trainable=False\n",
    "#base_model_3.trainable=False\n",
    "    \n",
    "model_3=Sequential()\n",
    "model_3.add(base_model_3)\n",
    "model_3.add(Flatten())\n",
    "\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model_3.add(Dense(256, activation='relu'))\n",
    "model_3.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_3.add(Dense(64, activation='relu'))\n",
    "model_3.add(BatchNormalization())\n",
    "model_3.add(Dropout(0.2))\n",
    "model_3.add(Dense(8,activation='softmax'))\n",
    "\n",
    "model_3.compile( optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 8 steps, validate for 4 steps\n",
      "Epoch 1/40\n",
      "8/8 [==============================] - 65s 8s/step - loss: 2.3236 - accuracy: 0.2638 - val_loss: 2.6014 - val_accuracy: 0.2909\n",
      "Epoch 2/40\n",
      "8/8 [==============================] - 68s 9s/step - loss: 1.6008 - accuracy: 0.5039 - val_loss: 3.5471 - val_accuracy: 0.2727\n",
      "Epoch 3/40\n",
      "8/8 [==============================] - 62s 8s/step - loss: 1.3293 - accuracy: 0.5079 - val_loss: 3.9274 - val_accuracy: 0.3727\n",
      "Epoch 4/40\n",
      "8/8 [==============================] - 62s 8s/step - loss: 1.0501 - accuracy: 0.6181 - val_loss: 5.3457 - val_accuracy: 0.2455\n",
      "Epoch 5/40\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.9524 - accuracy: 0.6811 - val_loss: 6.4507 - val_accuracy: 0.2545\n",
      "Epoch 6/40\n",
      "8/8 [==============================] - 67s 8s/step - loss: 0.8068 - accuracy: 0.7402 - val_loss: 4.8373 - val_accuracy: 0.2909\n",
      "Epoch 7/40\n",
      "8/8 [==============================] - 63s 8s/step - loss: 0.7806 - accuracy: 0.7441 - val_loss: 4.0964 - val_accuracy: 0.2818\n",
      "Epoch 8/40\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.6804 - accuracy: 0.7874 - val_loss: 3.1262 - val_accuracy: 0.3545\n",
      "Epoch 9/40\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.5408 - accuracy: 0.8346 - val_loss: 1.8238 - val_accuracy: 0.4455\n",
      "Epoch 10/40\n",
      "8/8 [==============================] - 64s 8s/step - loss: 0.5036 - accuracy: 0.8386 - val_loss: 1.2452 - val_accuracy: 0.6091\n",
      "Epoch 11/40\n",
      "8/8 [==============================] - 64s 8s/step - loss: 0.4219 - accuracy: 0.8701 - val_loss: 1.2856 - val_accuracy: 0.6091\n",
      "Epoch 12/40\n",
      "8/8 [==============================] - 62s 8s/step - loss: 0.3786 - accuracy: 0.8701 - val_loss: 1.3379 - val_accuracy: 0.6455\n",
      "Epoch 13/40\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.3397 - accuracy: 0.9094 - val_loss: 1.2631 - val_accuracy: 0.6636\n",
      "Epoch 14/40\n",
      "8/8 [==============================] - 62s 8s/step - loss: 0.3082 - accuracy: 0.9173 - val_loss: 1.2982 - val_accuracy: 0.6455\n",
      "Epoch 15/40\n",
      "8/8 [==============================] - 61s 8s/step - loss: 0.2552 - accuracy: 0.9213 - val_loss: 1.3456 - val_accuracy: 0.5727\n",
      "Epoch 16/40\n",
      "8/8 [==============================] - 62s 8s/step - loss: 0.2326 - accuracy: 0.9409 - val_loss: 1.1795 - val_accuracy: 0.6364\n",
      "Epoch 17/40\n",
      "8/8 [==============================] - 66s 8s/step - loss: 0.2556 - accuracy: 0.9291 - val_loss: 1.2005 - val_accuracy: 0.6545\n",
      "Epoch 18/40\n",
      "8/8 [==============================] - 66s 8s/step - loss: 0.1863 - accuracy: 0.9449 - val_loss: 0.9965 - val_accuracy: 0.7091\n",
      "Epoch 19/40\n",
      "8/8 [==============================] - 70s 9s/step - loss: 0.1512 - accuracy: 0.9606 - val_loss: 0.9225 - val_accuracy: 0.7091\n",
      "Epoch 20/40\n",
      "8/8 [==============================] - 76s 10s/step - loss: 0.2406 - accuracy: 0.9331 - val_loss: 0.9711 - val_accuracy: 0.7000\n",
      "Epoch 21/40\n",
      "8/8 [==============================] - 65s 8s/step - loss: 0.1442 - accuracy: 0.9724 - val_loss: 0.9311 - val_accuracy: 0.7091\n",
      "Epoch 22/40\n",
      "8/8 [==============================] - 65s 8s/step - loss: 0.1508 - accuracy: 0.9646 - val_loss: 0.9773 - val_accuracy: 0.7000\n",
      "Epoch 23/40\n",
      "8/8 [==============================] - 71s 9s/step - loss: 0.1634 - accuracy: 0.9646 - val_loss: 1.3038 - val_accuracy: 0.6636\n",
      "Epoch 24/40\n",
      "8/8 [==============================] - 73s 9s/step - loss: 0.1225 - accuracy: 0.9764 - val_loss: 1.1919 - val_accuracy: 0.7000\n",
      "Epoch 25/40\n",
      "8/8 [==============================] - 69s 9s/step - loss: 0.1381 - accuracy: 0.9803 - val_loss: 1.0565 - val_accuracy: 0.7364\n",
      "Epoch 26/40\n",
      "8/8 [==============================] - 65s 8s/step - loss: 0.1183 - accuracy: 0.9803 - val_loss: 0.9540 - val_accuracy: 0.7455\n",
      "Epoch 27/40\n",
      "8/8 [==============================] - 68s 8s/step - loss: 0.0898 - accuracy: 0.9882 - val_loss: 0.8946 - val_accuracy: 0.7727\n",
      "Epoch 28/40\n",
      "8/8 [==============================] - 65s 8s/step - loss: 0.1166 - accuracy: 0.9803 - val_loss: 0.8777 - val_accuracy: 0.7727\n",
      "Epoch 29/40\n",
      "8/8 [==============================] - 67s 8s/step - loss: 0.1215 - accuracy: 0.9843 - val_loss: 0.9154 - val_accuracy: 0.7364\n",
      "Epoch 30/40\n",
      "8/8 [==============================] - 71s 9s/step - loss: 0.0960 - accuracy: 0.9724 - val_loss: 0.9725 - val_accuracy: 0.7182\n",
      "Epoch 31/40\n",
      "8/8 [==============================] - 64s 8s/step - loss: 0.0733 - accuracy: 0.9882 - val_loss: 0.8801 - val_accuracy: 0.7455\n",
      "Epoch 32/40\n",
      "8/8 [==============================] - 68s 8s/step - loss: 0.0654 - accuracy: 0.9921 - val_loss: 1.0334 - val_accuracy: 0.7455\n",
      "Epoch 33/40\n",
      "8/8 [==============================] - 65s 8s/step - loss: 0.0947 - accuracy: 0.9724 - val_loss: 1.0920 - val_accuracy: 0.7545\n",
      "Epoch 34/40\n",
      "8/8 [==============================] - 63s 8s/step - loss: 0.0533 - accuracy: 0.9921 - val_loss: 1.1819 - val_accuracy: 0.7091\n",
      "Epoch 35/40\n",
      "8/8 [==============================] - 64s 8s/step - loss: 0.0835 - accuracy: 0.9724 - val_loss: 1.0664 - val_accuracy: 0.7273\n",
      "Epoch 36/40\n",
      "8/8 [==============================] - 63s 8s/step - loss: 0.0726 - accuracy: 0.9843 - val_loss: 1.0159 - val_accuracy: 0.7273\n",
      "Epoch 37/40\n",
      "8/8 [==============================] - 64s 8s/step - loss: 0.0646 - accuracy: 0.9882 - val_loss: 1.0552 - val_accuracy: 0.7364\n",
      "Epoch 38/40\n",
      "8/8 [==============================] - 65s 8s/step - loss: 0.0763 - accuracy: 0.9843 - val_loss: 0.9699 - val_accuracy: 0.7273\n",
      "Epoch 39/40\n",
      "8/8 [==============================] - 66s 8s/step - loss: 0.0793 - accuracy: 0.9803 - val_loss: 0.9836 - val_accuracy: 0.7182\n",
      "Epoch 40/40\n",
      "8/8 [==============================] - 65s 8s/step - loss: 0.0427 - accuracy: 0.9961 - val_loss: 0.9334 - val_accuracy: 0.7636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2a7bafbc978>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(train_datagen.flow(x_train, to_categorical(y_train,8), batch_size=32),epochs=40,\n",
    "          validation_data= valid_datagen.flow(x_valid, to_categorical(y_valid,8), batch_size=32),\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 2.8712414e-08 3.9486597e-35]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 8.0880691e-36 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 1.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 1.0000000e+00 2.0442083e-32 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00 0.0000000e+00]]\n",
      "['odissi', 'mohiniyattam', 'odissi']\n"
     ]
    }
   ],
   "source": [
    "labels = model_3.predict(test_img)\n",
    "print(labels[:4])\n",
    "label = [np.argmax(i) for i in labels]\n",
    "class_label = [inverse_map[x] for x in label]\n",
    "print(class_label[:3])\n",
    "submission = pd.DataFrame({ 'Image': test.Image, 'target': class_label })\n",
    "submission.head(10)\n",
    "submission.to_csv('submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manish Sehrawat\\anaconda3\\envs\\TensorFlow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'img_h' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4e4c54116208>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvgg19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVGG19\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbase_model_3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVGG19\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imagenet'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_h\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg_w\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpooling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'img_h' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19,preprocess_input\n",
    "base_model_3=VGG19(include_top=False, weights='imagenet',input_shape=(img_h,img_w,3), pooling='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
